{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport fasttext\n\nfrom torchtext.vocab import GloVe\n\nfrom torch.nn.utils.rnn import pad_sequence\nimport copy\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\nfrom typing_extensions import Literal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-26T16:07:04.377750Z","iopub.execute_input":"2023-03-26T16:07:04.378331Z","iopub.status.idle":"2023-03-26T16:07:04.712846Z","shell.execute_reply.started":"2023-03-26T16:07:04.378292Z","shell.execute_reply":"2023-03-26T16:07:04.711658Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCONFIG\n\"\"\"\n\ninput_dir = '/kaggle/input'\nglove_840b_path = f'{input_dir}/glove840b300dtxt/glove.840B.300d.txt'\nglove_6b_path = f'{input_dir}/glove6b300dtxt/glove.6B.300d.txt'\nfasttext_path = f'{input_dir}/fasttext-en-download/cc.en.300.bin'\ndata_path = f'{input_dir}/col772-a2-data'\n\nlr = 1e-4\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:04.714588Z","iopub.execute_input":"2023-03-26T16:07:04.714945Z","iopub.status.idle":"2023-03-26T16:07:04.780587Z","shell.execute_reply.started":"2023-03-26T16:07:04.714915Z","shell.execute_reply":"2023-03-26T16:07:04.779463Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class BioNERDataset(Dataset):\n    \n    def __init__(self, filename, label2id):\n        \n        self.sentences = []\n        self.token_counts = 0\n        self.class_counts = torch.zeros(len(label2id))\n        with open(filename, 'r') as f:\n            sentence = []\n            for l in f:\n                if l == '\\n':\n                    self.sentences.append(list(zip(*sentence)))\n                    sentence = []\n                else:\n                    token, cls = l.strip().split('\\t')\n                    sentence.append((token, label2id[cls]))\n                    self.class_counts[label2id[cls]] += 1\n                    self.token_counts += 1\n        \n        self.sentences.append(list(zip(*sentence)))\n        self.weights = self.token_counts / (self.class_counts*len(label2id))\n        \n    def __len__(self):\n        return len(self.sentences)\n        \n    def __getitem__(self, idx):\n        return self.sentences[idx]\n\ndef dl_collate_fn(data):\n    return tuple(zip(*data))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:05.097661Z","iopub.execute_input":"2023-03-26T16:07:05.098645Z","iopub.status.idle":"2023-03-26T16:07:05.108757Z","shell.execute_reply.started":"2023-03-26T16:07:05.098594Z","shell.execute_reply":"2023-03-26T16:07:05.107458Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class GloveModel(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n\n        ttext_glove = GloVe('6B')\n                    \n        # https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt\n        unk = torch.tensor([0.22418134, -0.28881392, 0.13854356, 0.00365387, \n                    -0.12870757, 0.10243822, 0.061626635, 0.07318011, \n                    -0.061350107, -1.3477012, 0.42037755, -0.063593924, \n                    -0.09683349, 0.18086134, 0.23704372, 0.014126852, \n                    0.170096, -1.1491593, 0.31497982, 0.06622181, \n                    0.024687296, 0.076693475, 0.13851812, 0.021302193, \n                    -0.06640582, -0.010336159, 0.13523154, -0.042144544, \n                    -0.11938788, 0.006948221, 0.13333307, -0.18276379, \n                    0.052385733, 0.008943111, -0.23957317, 0.08500333, \n                    -0.006894406, 0.0015864656, 0.063391194, 0.19177166, \n                    -0.13113557, -0.11295479, -0.14276934, 0.03413971, \n                    -0.034278486, -0.051366422, 0.18891625, -0.16673574, \n                    -0.057783455, 0.036823478, 0.08078679, 0.022949161, \n                    0.033298038, 0.011784158, 0.05643189, -0.042776518, \n                    0.011959623, 0.011552498, -0.0007971594, 0.11300405, \n                    -0.031369694, -0.0061559738, -0.009043574, -0.415336, \n                    -0.18870236, 0.13708843, 0.005911723, -0.113035575, \n                    -0.030096142, -0.23908928, -0.05354085, -0.044904727, \n                    -0.20228513, 0.0065645403, -0.09578946, -0.07391877, \n                    -0.06487607, 0.111740574, -0.048649278, -0.16565254, \n                    -0.052037314, -0.078968436, 0.13684988, 0.0757494, \n                    -0.006275573, 0.28693774, 0.52017444, -0.0877165, \n                    -0.33010918, -0.1359622, 0.114895485, -0.09744406, \n                    0.06269521, 0.12118575, -0.08026362, 0.35256687, \n                    -0.060017522, -0.04889904, -0.06828978, 0.088740796, \n                    0.003964443, -0.0766291, 0.1263925, 0.07809314, \n                    -0.023164088, -0.5680669, -0.037892066, -0.1350967, \n                    -0.11351585, -0.111434504, -0.0905027, 0.25174105, \n                    -0.14841858, 0.034635577, -0.07334565, 0.06320108, \n                    -0.038343467, -0.05413284, 0.042197507, -0.090380974, \n                    -0.070528865, -0.009174437, 0.009069661, 0.1405178, \n                    0.02958134, -0.036431845, -0.08625681, 0.042951006, \n                    0.08230793, 0.0903314, -0.12279937, -0.013899368, \n                    0.048119213, 0.08678239, -0.14450377, -0.04424887, \n                    0.018319942, 0.015026873, -0.100526, 0.06021201, \n                    0.74059093, -0.0016333034, -0.24960588, -0.023739101, \n                    0.016396184, 0.11928964, 0.13950661, -0.031624354, \n                    -0.01645025, 0.14079992, -0.0002824564, -0.08052984, \n                    -0.0021310581, -0.025350995, 0.086938225, 0.14308536, \n                    0.17146006, -0.13943303, 0.048792403, 0.09274929, \n                    -0.053167373, 0.031103406, 0.012354865, 0.21057427, \n                    0.32618305, 0.18015954, -0.15881181, 0.15322933, \n                    -0.22558987, -0.04200665, 0.0084689725, 0.038156632, \n                    0.15188617, 0.13274793, 0.113756925, -0.095273495, \n                    -0.049490947, -0.10265804, -0.27064866, -0.034567792, \n                    -0.018810693, -0.0010360252, 0.10340131, 0.13883452, \n                    0.21131058, -0.01981019, 0.1833468, -0.10751636, \n                    -0.03128868, 0.02518242, 0.23232952, 0.042052146, \n                    0.11731903, -0.15506615, 0.0063580726, -0.15429358, \n                    0.1511722, 0.12745973, 0.2576985, -0.25486213, \n                    -0.0709463, 0.17983761, 0.054027, -0.09884228, \n                    -0.24595179, -0.093028545, -0.028203879, 0.094398156, \n                    0.09233813, 0.029291354, 0.13110267, 0.15682974, \n                    -0.016919162, 0.23927948, -0.1343307, -0.22422817, \n                    0.14634751, -0.064993896, 0.4703685, -0.027190214, \n                    0.06224946, -0.091360025, 0.21490277, -0.19562101, \n                    -0.10032754, -0.09056772, -0.06203493, -0.18876675, \n                    -0.10963594, -0.27734384, 0.12616494, -0.02217992, \n                    -0.16058226, -0.080475815, 0.026953284, 0.110732645, \n                    0.014894041, 0.09416802, 0.14299914, -0.1594008, \n                    -0.066080004, -0.007995227, -0.11668856, -0.13081996, \n                    -0.09237365, 0.14741232, 0.09180138, 0.081735, \n                    0.3211204, -0.0036552632, -0.047030564, -0.02311798, \n                    0.048961394, 0.08669574, -0.06766279, -0.50028914, \n                    -0.048515294, 0.14144728, -0.032994404, -0.11954345, \n                    -0.14929578, -0.2388355, -0.019883996, -0.15917352, \n                    -0.052084364, 0.2801028, -0.0029121689, -0.054581646, \n                    -0.47385484, 0.17112483, -0.12066923, -0.042173345, \n                    0.1395337, 0.26115036, 0.012869649, 0.009291686, \n                    -0.0026459037, -0.075331464, 0.017840583, -0.26869613, \n                    -0.21820338, -0.17084768, -0.1022808, -0.055290595, \n                    0.13513643, 0.12362477, -0.10980586, 0.13980341, \n                    -0.20233242, 0.08813751, 0.3849736, -0.10653763, \n                    -0.06199595, 0.028849555, 0.03230154, 0.023856193, \n                    0.069950655, 0.19310954, -0.077677034, -0.144811])\n    \n        self.emb_dict = glove.stoi\n        self.unk_index = 400000\n        self.embedding = nn.Embedding.from_pretrained(torch.vstack([glove.vectors,unk]))\n    \n    def forward(self, sentence):\n        sentence = torch.tensor([self.emb_dict[word.lower()] if word.lower() in self.emb_dict else self.unk_index for word in sentence], device=device)\n        return self.embedding(sentence)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:08:22.587319Z","iopub.execute_input":"2023-03-26T16:08:22.587741Z","iopub.status.idle":"2023-03-26T16:08:22.620360Z","shell.execute_reply.started":"2023-03-26T16:08:22.587704Z","shell.execute_reply":"2023-03-26T16:08:22.619215Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class LSTMByteEmbeddingModel(nn.Module):\n    \n    def __init__(self, emb_dim=50):\n        super().__init__()\n        self.char_embeds  = nn.Embedding(256,emb_dim)\n        self.lstm = nn.LSTM(\n                input_size=emb_dim,\n                batch_first=True, \n                hidden_size=emb_dim,\n                bidirectional=True,\n            )\n        self.h0 = nn.Parameter(torch.randn(2,emb_dim))\n        self.c0 = nn.Parameter(torch.randn(2,emb_dim))\n    \n    def forward(self, sentence):\n        \n        x = pad_sequence([torch.tensor(bytearray(w.encode('utf-8'))).to(device) for w in sentence], batch_first=True)\n        x = self.char_embeds(x)\n        \n        batch_size = len(sentence)\n        h0 = torch.stack([self.h0 for _ in range(batch_size)], dim=1)\n        c0 = torch.stack([self.c0 for _ in range(batch_size)], dim=1)\n        _, (h, c) = self.lstm(x, (h0, c0))\n        return torch.cat((c[0], c[1]), 1)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:07.971931Z","iopub.execute_input":"2023-03-26T16:07:07.972976Z","iopub.status.idle":"2023-03-26T16:07:07.982966Z","shell.execute_reply.started":"2023-03-26T16:07:07.972914Z","shell.execute_reply":"2023-03-26T16:07:07.981720Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class EmbeddingModel(nn.Module):\n\n    # Embedding: 400\n    #  - Pretrained: 300\n    #  - Character: 100\n    #    - L: 50\n    #    - R: 50\n    #\n    # Tentative:\n    #  - POS: 25\n    #  - Chunk: 10\n    #  - Dict:  5 (may not be allowed)\n    def __init__(self):\n        super().__init__()\n        self.byte_emb_model = LSTMByteEmbeddingModel()            \n        self.word_emb_model = GloveModel()\n        self.dropout = nn.Dropout(p=0.2)\n    \n    def forward(self, sentence):\n        word_emb = self.word_emb_model(sentence)\n        byte_emb = self.byte_emb_model(sentence)        \n        return self.dropout(torch.cat([word_emb, byte_emb], 1))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:39.177319Z","iopub.execute_input":"2023-03-26T16:07:39.178025Z","iopub.status.idle":"2023-03-26T16:07:39.184426Z","shell.execute_reply.started":"2023-03-26T16:07:39.177987Z","shell.execute_reply":"2023-03-26T16:07:39.182990Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class ProjectiveAttention(nn.Module):\n    \n    def __init__(self, emb_dim = 800):\n        super().__init__()\n        self.W = nn.Parameter(torch.zeros((emb_dim, emb_dim)))\n        self.dropout = nn.Dropout(p=0.2)\n        nn.init.normal_(self.W.data, 0, 0.1)\n        \n    def forward(self, h):\n        # h: (B, N, K)\n        \n        # att = softmax(h@W@h_T, axis=1) @ h, across all batches\n        att = torch.bmm(\n                F.softmax(\n                    torch.bmm(\n                        h, \n                        torch.bmm(\n                            self.W.unsqueeze(0).expand([h.size(0),-1,-1]), \n                            h.transpose(2,1)\n                        )\n                    ), dim=2), \n                h\n              )\n        return self.dropout(att)\n\nclass PostAttentionLayer(nn.Module):\n    \n    def __init__(self, emb_dim = 1600, proj_dim = 800):\n        super().__init__()\n        self.fwd = nn.Linear(emb_dim, proj_dim, bias=False)\n        self.dropout = nn.Dropout(p=0.2)\n        \n    def forward(self, g, h):\n        return self.dropout(torch.tanh(self.fwd(torch.cat([g,h], dim=2))))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:41.503597Z","iopub.execute_input":"2023-03-26T16:07:41.504737Z","iopub.status.idle":"2023-03-26T16:07:41.514804Z","shell.execute_reply.started":"2023-03-26T16:07:41.504677Z","shell.execute_reply":"2023-03-26T16:07:41.513535Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class BiLSTMNERTagger(nn.Module):\n\n    def __init__(self, num_layers=2, embedding_size=400):\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(\n                input_size=embedding_size, \n                batch_first=True, \n                hidden_size=embedding_size, \n                bidirectional=True,\n                num_layers=self.num_layers,\n                dropout=0.2\n            )\n        self.pa = ProjectiveAttention(emb_dim=embedding_size*2)\n        self.pal = PostAttentionLayer(emb_dim=embedding_size*4, proj_dim = embedding_size*2)\n        self.clf = nn.Linear(2*embedding_size, 13)\n        self.lstm_final_dropout = nn.Dropout(p=0.2)\n\n        # TODO better initialization for this (closer to 0)\n        self.h0 = nn.Parameter(torch.zeros(2*self.num_layers,embedding_size))\n        self.c0 = nn.Parameter(torch.zeros(2*self.num_layers,embedding_size))\n        nn.init.normal_(self.h0.data, 0, 0.1)\n        nn.init.normal_(self.c0.data, 0, 0.1)\n        \n    def forward(self, x):\n\n        batch_size = x.size(0)\n        h = torch.stack([self.h0 for _ in range(batch_size)], dim=1)\n        c = torch.stack([self.c0 for _ in range(batch_size)], dim=1)\n        lstm_output, _ = self.lstm(x, (h,c))\n        lstm_dropout_output = self.lstm_final_dropout(lstm_output)\n        logits = self.clf(self.pal(lstm_dropout_output, self.pa(lstm_dropout_output)))\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:41.707255Z","iopub.execute_input":"2023-03-26T16:07:41.708045Z","iopub.status.idle":"2023-03-26T16:07:41.719306Z","shell.execute_reply.started":"2023-03-26T16:07:41.708002Z","shell.execute_reply":"2023-03-26T16:07:41.718313Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def preprocess_batch(emb_model, sentences, labels=None):\n    \n    pt_sents = []\n    pt_labels = []\n    pt_masks = []\n    i = 0\n    for i in range(len(sentences)):\n        emb = emb_model(sentences[i])\n        pt_sents.append(emb)\n        if labels:\n            pt_labels.append(torch.tensor(labels[i]))\n        pt_masks.append(torch.tensor([1]*emb.shape[0]))\n        \n        i += 1\n        \n    if labels:\n        return pad_sequence(pt_sents, batch_first=True), pad_sequence(pt_labels, batch_first=True, padding_value=-100), (pad_sequence(pt_masks, batch_first=True) > 0)\n    \n    return pad_sequence(pt_sents, batch_first=True), (pad_sequence(pt_masks, batch_first=True) > 0)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:41.930491Z","iopub.execute_input":"2023-03-26T16:07:41.930762Z","iopub.status.idle":"2023-03-26T16:07:41.937932Z","shell.execute_reply.started":"2023-03-26T16:07:41.930737Z","shell.execute_reply":"2023-03-26T16:07:41.936851Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"label2id = {'O': 0,\n 'B-Biological_Molecule': 1,\n 'E-Biological_Molecule': 2,\n 'S-Biological_Molecule': 3,\n 'I-Biological_Molecule': 4,\n 'B-Species': 5,\n 'E-Species': 6,\n 'S-Species': 7,\n 'I-Species': 8,\n 'B-Chemical_Compound': 9,\n 'E-Chemical_Compound': 10,\n 'S-Chemical_Compound': 11,\n 'I-Chemical_Compound': 12}","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:42.441720Z","iopub.execute_input":"2023-03-26T16:07:42.442017Z","iopub.status.idle":"2023-03-26T16:07:42.447984Z","shell.execute_reply.started":"2023-03-26T16:07:42.441989Z","shell.execute_reply":"2023-03-26T16:07:42.446799Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import random\nseed = 3143\ntorch.manual_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:43.141626Z","iopub.execute_input":"2023-03-26T16:07:43.142468Z","iopub.status.idle":"2023-03-26T16:07:43.152243Z","shell.execute_reply.started":"2023-03-26T16:07:43.142421Z","shell.execute_reply":"2023-03-26T16:07:43.151301Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# train\ntrain_ds = BioNERDataset(f'{data_path}/train.txt', label2id)\nval_ds = BioNERDataset(f'{data_path}/dev.txt', label2id)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:07:43.643518Z","iopub.execute_input":"2023-03-26T16:07:43.644332Z","iopub.status.idle":"2023-03-26T16:08:00.388359Z","shell.execute_reply.started":"2023-03-26T16:07:43.644302Z","shell.execute_reply":"2023-03-26T16:08:00.387329Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"DEBUG = False\nif DEBUG:\n    train_ds.sentences = train_ds.sentences[:128]\n    val_ds.sentences = val_ds.sentences[:32]","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:08:00.390361Z","iopub.execute_input":"2023-03-26T16:08:00.390716Z","iopub.status.idle":"2023-03-26T16:08:00.397374Z","shell.execute_reply.started":"2023-03-26T16:08:00.390687Z","shell.execute_reply":"2023-03-26T16:08:00.394414Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size=16, collate_fn=dl_collate_fn, num_workers=2, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=16, collate_fn=dl_collate_fn, num_workers=2, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:08:00.399023Z","iopub.execute_input":"2023-03-26T16:08:00.399371Z","iopub.status.idle":"2023-03-26T16:08:00.409643Z","shell.execute_reply.started":"2023-03-26T16:08:00.399335Z","shell.execute_reply":"2023-03-26T16:08:00.408726Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = BiLSTMNERTagger(num_layers=2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:08:00.412384Z","iopub.execute_input":"2023-03-26T16:08:00.413122Z","iopub.status.idle":"2023-03-26T16:08:05.100694Z","shell.execute_reply.started":"2023-03-26T16:08:00.413032Z","shell.execute_reply":"2023-03-26T16:08:05.099657Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"emb_model = EmbeddingModel().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:08:25.619734Z","iopub.execute_input":"2023-03-26T16:08:25.620298Z","iopub.status.idle":"2023-03-26T16:08:26.878602Z","shell.execute_reply.started":"2023-03-26T16:08:25.620259Z","shell.execute_reply":"2023-03-26T16:08:26.877448Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(list(model.parameters())+list(emb_model.parameters()), lr=1e-3)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\nloss_fn = nn.CrossEntropyLoss(weight=train_ds.weights.to(device))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:08:34.287670Z","iopub.execute_input":"2023-03-26T16:08:34.288386Z","iopub.status.idle":"2023-03-26T16:08:34.295629Z","shell.execute_reply.started":"2023-03-26T16:08:34.288344Z","shell.execute_reply":"2023-03-26T16:08:34.294610Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"best_model = None\nbest_emb_model = None\nf1_means = []\nbest_val_loss = 10000\nbest_f1_mean = 0\nval_losses = []\ntrain_losses = []\npossible_labels = range(1,13)\n\nfor epoch in range(30):\n\n    print(f'Epoch {epoch+1}:')\n    train_loss = torch.tensor(0, dtype=torch.float, device=device)\n    model.train()\n    for (sentences, answers) in tqdm(train_dl):\n        tokens, labels, masks = preprocess_batch(emb_model, sentences, labels=answers)\n        tokens = tokens.to(device)\n        labels = labels.to(device).flatten()\n\n        optimizer.zero_grad()\n        logits = model(tokens).flatten(end_dim=1)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.detach()\n    \n    scheduler.step()\n\n    train_loss = train_loss.cpu()\n    train_loss /= len(train_dl)\n    print(f' Train Loss: {train_loss}')\n    train_losses.append(train_loss)\n\n    val_loss = torch.tensor(0, dtype=torch.float, device=device)\n    true_labels = []\n    pred_labels = []\n    model.eval()\n    for (sentences, answers) in tqdm(val_dl):\n        tokens, labels, mask = preprocess_batch(emb_model, sentences, labels=answers)\n        tokens = tokens.to(device)\n        labels = labels.to(device).flatten()\n        mask = mask.to(device).flatten()\n\n        logits = model(tokens).flatten(end_dim=1)\n        loss = loss_fn(logits, labels)\n\n        val_loss += loss.detach()\n        \n        pred_labels.append(torch.argmax(logits, axis=1)[mask])\n        true_labels.append(labels[mask])\n    \n    val_loss = val_loss.cpu()\n    val_loss /= len(val_dl)\n    val_losses.append(val_loss)\n    \n    pred_labels = np.array(torch.hstack(pred_labels).cpu())\n    true_labels = np.array(torch.hstack(true_labels).cpu())\n\n    f1_micro = f1_score(true_labels, pred_labels, average=\"micro\", labels=possible_labels)\n    f1_macro = f1_score(true_labels, pred_labels, average=\"macro\", labels=possible_labels)\n\n    f1_mean = (f1_macro + f1_micro)/2\n    f1_means.append(f1_mean)\n\n    print(f' Val Loss: {val_loss}')\n    print(f' F1 micro: {f1_micro}')\n    print(f' F1 macro: {f1_macro}')\n    print(f' F1 mean: {f1_mean}')\n    print('')\n    \n    # early stopping\n    if f1_mean <= best_f1_mean:\n        if patience >= 4:\n            break\n        else:\n            patience += 1\n    else:\n        patience = 0\n        best_val_loss = val_loss\n        best_f1_mean = f1_mean\n        best_model = copy.deepcopy(model)\n        best_emb_model = copy.deepcopy(emb_model)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:08:35.189040Z","iopub.execute_input":"2023-03-26T16:08:35.189805Z","iopub.status.idle":"2023-03-26T16:09:05.210113Z","shell.execute_reply.started":"2023-03-26T16:08:35.189766Z","shell.execute_reply":"2023-03-26T16:09:05.207838Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2401 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffb0f91293d4baeab38508caf28a0ce"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/4104348478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1700130061.py\u001b[0m in \u001b[0;36mpreprocess_batch\u001b[0;34m(emb_model, sentences, labels)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mpt_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1502812009.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mword_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_emb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mbyte_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_emb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_emb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/846538267.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/846538267.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(f'Best F1 mean: {best_f1_mean}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_model, '/kaggle/working/best_model.pt')\ntorch.save(best_emb_model, '/kaggle/working/best_emb_model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-03-20T16:14:59.655331Z","iopub.status.idle":"2023-03-20T16:14:59.656736Z","shell.execute_reply.started":"2023-03-20T16:14:59.656422Z","shell.execute_reply":"2023-03-20T16:14:59.656459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(train_losses)\nplt.plot(val_losses)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T16:14:59.658200Z","iopub.status.idle":"2023-03-20T16:14:59.659061Z","shell.execute_reply.started":"2023-03-20T16:14:59.658797Z","shell.execute_reply":"2023-03-20T16:14:59.658823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_losses)\nplt.plot(f1_means)","metadata":{},"execution_count":null,"outputs":[]}]}