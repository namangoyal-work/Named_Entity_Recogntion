{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14f0ded6-a90e-427d-9c2e-cb087febacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "59b36cf5-51f3-4b4a-b9f9-5dce241bdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.set_start_method('fork', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c621f65-84f9-42a0-b26e-60e605d67197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PARAMETERS\n",
    "\"\"\"\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef25f389-d19c-4e02-9ece-5c627ab5c2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9ef6e3e5-8518-4c07-9a23-712505ffdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is ok, need to think of an encoding scheme now...\n",
    "class NERDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filename, label2id):\n",
    "        \n",
    "        self.sentences = []\n",
    "        with open(filename, 'r') as f:\n",
    "            sentence = []\n",
    "            for l in f:\n",
    "                if l == '\\n':\n",
    "                    self.sentences.append(zip(*sentence))\n",
    "                    sentence = []\n",
    "                else:\n",
    "                    token, cls = l.strip().split('\\t')\n",
    "                    sentence.append((token, label2id[cls]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "175becd5-9948-4c49-a337-852e408158d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_collate_fn(data):\n",
    "    return tuple(zip(*data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3d97d7f-dedf-40b0-9ddd-de45a44ffe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token alignment\n",
    "def tokenize_and_align_labels(tokenizer, batch):\n",
    "\n",
    "    tokenized_inputs = tokenizer(batch[0], padding=True, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(batch[1]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels, device=device)\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1eeb9a-2e44-4e16-a35d-41357a67d944",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c24d2ee1-18c6-425c-a5ec-538cdc20d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O': 0,\n",
    " 'B-Biological_Molecule': 1,\n",
    " 'E-Biological_Molecule': 2,\n",
    " 'S-Biological_Molecule': 3,\n",
    " 'I-Biological_Molecule': 4,\n",
    " 'S-Species': 5,\n",
    " 'B-Species': 6,\n",
    " 'I-Species': 7,\n",
    " 'E-Species': 8,\n",
    " 'B-Chemical_Compound': 9,\n",
    " 'E-Chemical_Compound': 10,\n",
    " 'S-Chemical_Compound': 11,\n",
    " 'I-Chemical_Compound': 12}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "51b70730-9e83-4769-8c23-da53f0090dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NERDataset('train.txt', label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c3dfa81-fbb6-421d-8e0e-843ba7f9f8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_ds = NERDataset('dev.txt', label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d04d8134-c2e1-4f4c-825f-812f1a2b9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=8, collate_fn=dl_collate_fn, num_workers=4, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=8, collate_fn=dl_collate_fn, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a04cf71c-7207-425d-bbf7-598bf5d411cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5682f98a206140cfbfa3d3e839aeb64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "model = transformers.AutoModelForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=13, id2label=id2label, label2id=label2id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c615806-088b-453a-b04e-2397199e50c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4801"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f3405d20-e25f-4b25-8eca-c5efde99df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adeab8f910e54af8acc43215a5482f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl):\n\u001b[1;32m      9\u001b[0m     tok_batch \u001b[38;5;241m=\u001b[39m tokenize_and_align_labels(tokenizer, batch)\n\u001b[0;32m---> 10\u001b[0m     tok_batch \u001b[38;5;241m=\u001b[39m {k : v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tok_batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtok_batch)\u001b[38;5;241m.\u001b[39mloss\n",
      "Cell \u001b[0;32mIn[102], line 10\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl):\n\u001b[1;32m      9\u001b[0m     tok_batch \u001b[38;5;241m=\u001b[39m tokenize_and_align_labels(tokenizer, batch)\n\u001b[0;32m---> 10\u001b[0m     tok_batch \u001b[38;5;241m=\u001b[39m {k : \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tok_batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtok_batch)\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dl):\n",
    "        tok_batch = tokenize_and_align_labels(tokenizer, batch)\n",
    "        tok_batch = {k : v.to(device) for k, v in tok_batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(**tok_batch).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.detach()\n",
    "    \n",
    "    train_loss = train_loss.cpu()\n",
    "    train_loss /= len(train_dl)\n",
    "    print(f' Train Loss: {train_loss}')\n",
    "        \n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for batch in tqdm(val_dl):\n",
    "        tok_batch = tokenize_and_align_labels(tokenizer, batch)\n",
    "        tok_batch = {k : v.to(device) for k, v in tok_batch.items()}\n",
    "        \n",
    "        loss = model(**tok_batch).loss\n",
    "        val_loss += loss.detach()\n",
    "\n",
    "    val_loss = val_loss.cpu()\n",
    "    val_loss /= len(val_dl)\n",
    "    print(f' Val Loss: {val_loss}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e5843-3628-48d5-9ae9-1b8faeffc048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
